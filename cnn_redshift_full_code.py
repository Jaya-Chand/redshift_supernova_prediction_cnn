# -*- coding: utf-8 -*-
"""CNN Redshift FULL CODE

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s39HE_d3YDuv7RKi3xoSPjeKYY69ETjL
"""

##importing relevant libraries
!pip install --quiet astrodash
import numpy as np
import matplotlib.pyplot as plt
import astrodash
import random
import math
import os
import os.path

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D

##mounting the drive so astrodash templates can be accessed
from google.colab import drive
drive.mount('/content/drive')

def find_all_files_in_directory(path):
  filenames=[]
  for filename in os.listdir(path): 
    filenames.append(filename)
  return filenames

###Getting ages
def getting_age(filename):
  age=filename.split('_')[0]
  return float(age)

from google.colab import drive
drive.mount('/content/drive')

##Clearing the folder
def clearing_folder(path):
  filesToRemove = [os.path.join(path,f) for f in os.listdir(path)]
  for f in filesToRemove:
      os.remove(f)

##redshifting spectra using astrodash
def redshifting(fluxes, redshift, nIndexes, dwlog, w0, w1, nw):
    rbs_fluxes=astrodash.helpers.redshift_binned_spectrum(fluxes, redshift, nIndexes, dwlog, w0, w1, nw, outerVal=0.0)
    return rbs_fluxes

##extracting the data from the datafile
def extracting_data_from_file(path,filename):
    data=np.loadtxt(path+filename)
    wavelengths=data[:,0]
    fluxes=data[:,1]
    return wavelengths, fluxes

def redshift_value():
  redshift=random.uniform(0,8)/10
  return redshift

def force_float(string_array):
  for i in range(len(string_array)):
    string_array[i]=float(string_array[i])
  return string_array

##ordering the Files by age so that we can limit it to -2 and 2 days
#print(filenames_1a)
#print(sorted(filenames_1a))

def constraining_ages(lower_bound,upper_bound,filenames_1a):
  subset_filenames_1a=[]
  
  for file in filenames_1a:
    age=getting_age(file)
    if age>=lower_bound and age<=upper_bound:
      subset_filenames_1a.append(file)
  
  #subset_filenames_1a=sorted(subset_filenames_1a)
  
  return subset_filenames_1a

##Reading Supernova 1a z=0 data from the drive
lnw_path="/content/drive/MyDrive/SupernovaDM_binned_age_spectra/Type1a_2column/"
filenames_1a=find_all_files_in_directory(lnw_path)

##picking a random spectra 

filenames_constrained=constraining_ages(-15,40,filenames_1a)
print(f"There are {len(filenames_constrained)} total files in the list")
random.shuffle(filenames_constrained)
total=len(filenames_constrained)

selected_spectra="//content/drive/MyDrive/SupernovaDM_binned_age_spectra/selected"

total=len(filenames_constrained)

files=filenames_constrained

##Randomly choosing the validation, training and testing set
##10:20:70
validation_amount=int((total*10)/100)
testing_amount=int((total*30)/100)

validation_data=files[:validation_amount]
print(f"There are {len(validation_data)} files in the validation set")
print(validation_data)

testing_data = files[validation_amount:testing_amount]
print(f"There are {len(testing_data)} files in the testing set")
print(testing_data)

training_data = files[testing_amount:]
print(f"There are {len(training_data)} files in the training set")
print(training_data)

print(len(validation_data)+len(testing_data)+len(training_data))

##########
w0=3000
w1=10000
nw=1024
dwlog = np.log(w1/w0)/nw
nIndexes= np.arange(0,nw)
###########

x_train=[]
y_train=[]

x_test=[]
y_test=[]

###############

redshifted_training_path="//content/drive/MyDrive/SupernovaDM_binned_age_spectra/training_set/"
redshifted_testing_path="//content/drive/MyDrive/SupernovaDM_binned_age_spectra/testing_set/"
redshifted_validation_path="//content/drive/MyDrive/SupernovaDM_binned_age_spectra/validation_set/"

clearing_folder(redshifted_training_path)
clearing_folder(redshifted_testing_path)
clearing_folder(redshifted_validation_path)

num_shifts=50

raw_data_path="/content/drive/MyDrive/SupernovaDM_binned_age_spectra/Type1a_2column/"

for validation_filename in validation_data:

  for shift in range(num_shifts):   

    wavelengths, fluxes=extracting_data_from_file(raw_data_path,validation_filename)
                                                  
    redshift=redshift_value()

    ##redshifting the training set
    redshifted_fluxes=redshifting(fluxes, redshift, nIndexes, dwlog, w0, w1, nw)


for training_filename in training_data:

  for shift in range(num_shifts): 

    wavelengths, fluxes=extracting_data_from_file(raw_data_path,training_filename)
  
    redshift=redshift_value()

    ##redshifting the training set
    redshifted_fluxes=redshifting(fluxes, redshift, nIndexes, dwlog, w0, w1, nw)

    ##training x will be the supernova flux
    x_train.append(force_float(redshifted_fluxes))

    ##training y will be the associated redshift
    y_train.append(redshift)
 
x_train=np.array(x_train)
y_train=force_float(y_train)

for testing_filename in testing_data:
  
  for shift in range(num_shifts): 

    wavelengths, fluxes=extracting_data_from_file(raw_data_path,testing_filename)

    redshift=redshift_value()

    ##redshifting the training set
    redshifted_fluxes=redshifting(fluxes, redshift, nIndexes, dwlog, w0, w1, nw)

    ##training x will be the supernova flux
    x_test.append(force_float(redshifted_fluxes))

    ##training y will be the associated redshift
    y_test.append(redshift)

x_test=np.array(x_test)
y_test=force_float(y_test)

##PREPARING DATA

# Model / data parameters

input_shape = (1024)

# Scaling the spectra to the [0, 1] range
x_train = x_train.astype("float32") / 15
x_test = x_test.astype("float32") / 15

#plt.plot(x_train)

##maintaining relative numbers if dividing by all at end
##could normalise EACH individual spectra
##TO DO: make it so from 0-1, add minimum to all values 

# Make sure images have shape (28, 28, 1)
#x_train = np.expand_dims(x_train, -1)
#x_test = np.expand_dims(x_test, -1)
print("x_train shape:", x_train.shape)
print("x_test shape:", x_test.shape)
print(x_train.shape[0], "train samples")
print(x_test.shape[0], "test samples")

##reshaping the x_training and x_testing
x_train=x_train.reshape((x_train.shape[0],x_train.shape[1],-1))
print(x_train.shape)

x_test=x_test.reshape((x_test.shape[0],x_test.shape[1],-1))
print(x_test.shape)

##number of examples, timesteps, features

n_timesteps=1
n_features=1024
n_outputs=1

model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu',))
model.add(Dropout(0.0))
model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(n_outputs))
model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])

##TRAINING THE MODEL
##batch sizes, uses 64
##epochs is how many times its repeating choosing batch_size number from spectra

batch_size = 200
epochs = 20

model.compile(loss="mse", optimizer="adam", metrics=["accuracy"])

x_train=np.array(x_train)
y_train=np.array(y_train)

history=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1,validation_data=(x_test,y_test))

print(history)
##history should be dictionary
##get loss from it

# Get training and test loss histories
print(history.history.keys())
training_loss = history.history['loss']
test_loss = history.history['val_loss']
train_acc  = history.history['acc']
val_acc    = history.history['val_acc']

# Create count of the number of epochs
epoch_count = range(1, len(training_loss) + 1)

# Visualize loss history
plt.figure(figsize=(10,5))
plt.title("Plot of how the test and training loss change with epoch",size=18)
plt.plot(epoch_count, training_loss, 'r--')
plt.plot(epoch_count, test_loss, 'b--')
plt.legend(['Training Loss', 'Test Loss'])
plt.xlabel('Epoch',size=16)
plt.ylabel('Loss',size=16)
plt.show();

# Visualize loss history
plt.figure(figsize=(10,5))
plt.title("Plot of how the test and training error change with epoch",size=18)

training_error=np.ones(len(training_loss))
testing_error=np.ones(len(test_loss))

for i in range(len(training_loss)):
  training_error[i]=(training_loss[i])**0.5

for i in range (len(test_loss)):
  testing_error[i]=(test_loss[i])**0.5

plt.plot(epoch_count,training_error , 'r--')
plt.plot(epoch_count,testing_error , 'b--')
plt.legend(['Training Error', 'Test Error'])
plt.xlabel('Epoch',size=16)
plt.ylabel('Root Mean Squared Loss',size=16)
plt.show();

##Visualising accuracy history
plt.figure(figsize=(10,5))
plt.title("Plot of how the test and training accuracy change with epoch",size=18)
plt.plot(epoch_count, train_acc, 'r--')
plt.plot(epoch_count, val_acc , 'b--')
plt.legend(['Training Accuracy', 'Test Accuracy'])
plt.xlabel('Epoch',size=16)
plt.ylabel('Loss',size=16)
plt.show();

##EVALUATING THE MODEL

print(type(x_test[0]))
print(type(y_test[0]))

y_test=np.array(y_test)

score = model.evaluate(x_test, y_test, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

y_pred_testing=model.predict(x_test)
y_pred_training=model.predict(x_train)


title="Plot of the true training set redshifts against the prediction values"
plt.figure(figsize=(10,7))
plt.plot(y_test,y_pred_testing,'g.',label="Plot of the testing prediction redshift values against the true redshift values")
#plt.plot(y_train,y_pred_training,'b.')
plt.plot(np.linspace(0,0.8),np.linspace(0,0.8),'r',label="Ideal model performance")
plt.title(title, fontsize=12)
plt.xlabel('True Redshift Value', fontsize=12)
plt.ylabel("CNN Predicted Redshift Value", fontsize=12)  
plt.legend(loc="best") 
plt.show()

y_pred_testing=model.predict(x_test)
y_pred_training=model.predict(x_train)

y_test=force_float(list(y_test))
y_pred_testing=force_float(list(y_pred_testing))
residuals=np.zeros(len(y_test))

for i in range(len(y_test)-1):
  residuals[i]=y_pred_testing[i]-y_test[i]

  
title="Plot of the residuals between true training set redshifts against the prediction values"
plt.figure(figsize=(20,7))

plt.plot(y_test,residuals,'b.',label="Residuals for each true redshift")
plt.plot(np.linspace(0,0.8,20),np.zeros(20),"k--")

coeffs=np.polyfit(y_test,residuals,17)
poly=np.poly1d(coeffs)
new_x=np.linspace(0,0.5,1000,6)
new_y= poly(new_x)
#plt.plot(new_x,new_y,'r',label="Fitted polynomial")

plt.title(title, fontsize=16)
plt.xlabel('True Redshift Value', fontsize=14)
plt.ylabel("Residuals (Predicted-True Redshift)", fontsize=14)  
plt.legend(loc="best", fontsize=12) 
plt.show()

title="Plot of the distribution of residuals"
plt.figure(figsize=(20,7))
plt.hist(residuals,bins=25)
plt.title(title, fontsize=16)
plt.xlabel('Residuals', fontsize=14)
plt.ylabel("Frequency", fontsize=14)  
plt.legend(loc="best") 
plt.show()

print(f"The mean is {np.mean(residuals)}")
print(f"The standard deviation is {np.std(residuals)}")

##saving to files

np.savetxt('epoch_count_1DCNN.txt', epoch_count, fmt='%d')

np.savetxt('train_acc_1DCNN.txt',train_acc, fmt='%d')

np.savetxt('val_acc_1DCNN.txt',val_acc, fmt='%d')

np.savetxt('training_error_1DCNN.txt',training_error, fmt='%d')

np.savetxt('testing_error_1DCNN.txt',testing_error, fmt='%d')

np.savetxt('y_test_1DCNN.txt',y_test, fmt='%d')

np.savetxt('y_test_1DCNN.txt',y_test, fmt='%d')

np.savetxt('y_pred_testing_1DCNN.txt',y_pred_testing, fmt='%d')

np.savetxt('residuals_1DCNN.txt',residuals, fmt='%d')